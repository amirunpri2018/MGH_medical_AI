{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After patient data has been queried, we are moving to model practise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://github.com/YerevaNN/mimic3-benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Johnson, Alistair EW, David J. Stone, Leo A. Celi, and Tom J. Pollard. \"The MIMIC Code Repository: enabling reproducibility in critical care research.\" Journal of the American Medical Informatics Association (2017): ocx084.\n",
    "* Github: https://github.com/MIT-LCP/mimic-code\n",
    "* Zenodo: https://doi.org/10.5281/zenodo.821872\n",
    "* Structure tested: multitask RNN architecture: https://arxiv.org/abs/1703.07771 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hrayr Harutyunyan, Hrant Khachatrian, David C. Kale, and Aram Galstyan. Multitask Learning and Benchmarking with Clinical Time Series Data. arXiv:1703.07771: https://arxiv.org/abs/1703.07771\n",
    "<br>\n",
    "\n",
    "* Mimic3 benchmarks for machine learning: https://github.com/YerevaNN/mimic3-benchmarks\n",
    "    1. early triage and risk assessment, i.e., mortality prediction\n",
    "    2. prediction of physiologic decompensation\n",
    "    3. identification of high cost patients, i.e. length of stay forecasting\n",
    "    4. characterization of complex, multi-system diseases, i.e., acute care phenotyping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ericx\\AppData\\Local\\conda\\conda\\envs\\PY36\\lib\\site-packages\\h5py\\__init__.py:72: UserWarning: h5py is running against HDF5 1.10.2 when it was built against 1.10.3, this may cause problems\n",
      "  '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn                              # generic machine learning tool kit\n",
    "import keras                                # for LSTM model to handle time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ericx\\\\Jupyter Projects\\\\MGH_medical_AI'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from packages import directory_tree         # note this is a package I wrote to display directory trees\n",
    "import os\n",
    "CURRENT_DIR = os.path.abspath('')\n",
    "CURRENT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data prepared for ML\n",
    "\n",
    "Note: The whole process might take a few hours..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Clone the repo. Following code will create ~/data folder to contain ML train/test/val\n",
    "\n",
    "`git clone https://github.com/YerevaNN/mimic3-benchmarks/\n",
    "cd mimic3-benchmarks/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. The following command takes MIMIC-III CSVs, the steps takes around ~3 hour.\n",
    "- generates one directory per `SUBJECT_ID`, totally 33798 folders unnder ~/data/root/\n",
    "- writes ICU stay information to `data/{SUBJECT_ID}/stays.csv`\n",
    "- diagnoses to `data/{SUBJECT_ID}/diagnoses.csv`\n",
    "- and events to `data/{SUBJECT_ID}/events.csv`. \n",
    "\n",
    "`python -m mimic3benchmark.scripts.extract_subjects {PATH TO MIMIC-III CSVs} data/root/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. The following command attempts to fix some issues (ICU stay ID is missing) and removes the events that have missing information. About 80% of events remain after removing all suspicious rows.\n",
    "\n",
    "`python -m mimic3benchmark.scripts.validate_events data/root/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. The next command breaks up per-subject data into separate episodes (pertaining to ICU stays). \n",
    "- Time series of events are stored in `{SUBJECT_ID}/episode{#}_timeseries.csv` (where # counts distinct episodes) \n",
    "- episode-level information (patient age, gender, ethnicity, height, weight) and outcomes (mortality, length of stay, diagnoses) are stores in `{SUBJECT_ID}/episode{#}.csv`. \n",
    "- This script requires two files, one that maps event `ITEMIDs` to clinical variables and another that defines valid ranges for clinical variables (for detecting outliers, etc.). \n",
    "- Outlier detection is disabled in the current version.\n",
    "\n",
    "`python -m mimic3benchmark.scripts.extract_episodes_from_subjects data/root/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. The next command splits the whole dataset into training and testing sets. Note that the train/test split is the same of all tasks.\n",
    "\n",
    "`python -m mimic3benchmark.scripts.split_train_and_test data/root/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. The following commands will generate task-specific datasets, which can later be used in models. These commands are independent, if you are going to work only on one benchmark task, you can run only the corresponding command.\n",
    "\n",
    "\n",
    "1. early triage and risk assessment, i.e., mortality prediction <br>\n",
    "`python -m mimic3benchmark.scripts.create_in_hospital_mortality data/root/ data/in-hospital-mortality/`\n",
    "<br>\n",
    "    \n",
    "2. prediction of physiologic decompensation<br>\n",
    "`python -m mimic3benchmark.scripts.create_decompensation data/root/ data/decompensation/`\n",
    "<br>\n",
    "\n",
    "3. identification of high cost patients, i.e. length of stay forecasting<br>\n",
    "`python -m mimic3benchmark.scripts.create_length_of_stay data/root/ data/length-of-stay/`\n",
    "<br>\n",
    "\n",
    "4. characterization of complex, multi-system diseases, i.e., acute care phenotyping<br>\n",
    "`python -m mimic3benchmark.scripts.create_phenotyping data/root/ data/phenotyping/`<br>\n",
    "`python -m mimic3benchmark.scripts.create_multitask data/root/ data/multitask/`\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. After the above commands are done, there will be a directory data/{task} for each created benchmark task. These directories have two sub-directories: `train` and `test`. Each of them contains bunch of ICU stays and one file with name `listfile.csv`, which lists all samples in that particular set. \n",
    "\n",
    "Each row of listfile.csv has the following form: `icu_stay`, `period_length`, `label(s)`. A row specifies a sample for which the input is the collection of ICU event of `icu_stay` that occurred in the first `period_length` hours of the stay and the target is/are label(s). \n",
    "\n",
    "In in-hospital mortality prediction task `period_length` is always 48 hours, so it is not listed in corresponding listfiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. Seems some record missing for mortality data<br>\n",
    "Note remaining tasks also have data missing, won't copy all details.\n",
    "\n",
    "`(PY36) $ python -m mimic3benchmark.scripts.create_in_hospital_mortality data/root/ data/in-hospital-mortality/\n",
    "processed 5000 / 5070 patients\n",
    " 3236\n",
    "(length of stay is missing) 10128 episode1_timeseries.csv\n",
    "processed 100 / 28728 patients\n",
    "(length of stay is missing) 10168 episode1_timeseries.csv\n",
    "processed 2400 / 28728 patients\n",
    "(no events in ICU)  14219 episode1_timeseries.csv\n",
    "processed 2600 / 28728 patients\n",
    "(no events in ICU)  14469 episode1_timeseries.csv\n",
    "processed 4700 / 28728 patients\n",
    "(no events in ICU)  18350 episode1_timeseries.csv\n",
    "processed 5200 / 28728 patients\n",
    "(no events in ICU)  19097 episode1_timeseries.csv\n",
    "processed 5600 / 28728 patients\n",
    "(no events in ICU)  19872 episode1_timeseries.csv\n",
    "processed 16100 / 28728 patients\n",
    "(length of stay is missing) 499 episode1_timeseries.csv\n",
    "processed 28700 / 28728 patients\n",
    " 17903`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dirlist = directory_tree.get_dir_list('./')\n",
    "print(dirlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
