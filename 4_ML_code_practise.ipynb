{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After patient data has been queried, we are moving to model practise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://github.com/YerevaNN/mimic3-benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Johnson, Alistair EW, David J. Stone, Leo A. Celi, and Tom J. Pollard. \"The MIMIC Code Repository: enabling reproducibility in critical care research.\" Journal of the American Medical Informatics Association (2017): ocx084.\n",
    "* Github: https://github.com/MIT-LCP/mimic-code\n",
    "* Zenodo: https://doi.org/10.5281/zenodo.821872\n",
    "* Structure tested: multitask RNN architecture: https://arxiv.org/abs/1703.07771 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hrayr Harutyunyan, Hrant Khachatrian, David C. Kale, and Aram Galstyan. Multitask Learning and Benchmarking with Clinical Time Series Data. arXiv:1703.07771: https://arxiv.org/abs/1703.07771\n",
    "<br>\n",
    "\n",
    "* Mimic3 benchmarks for machine learning: https://github.com/YerevaNN/mimic3-benchmarks\n",
    "    1. early triage and risk assessment, i.e., mortality prediction\n",
    "    2. prediction of physiologic decompensation\n",
    "    3. identification of high cost patients, i.e. length of stay forecasting\n",
    "    4. characterization of complex, multi-system diseases, i.e., acute care phenotyping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn                              # generic machine learning tool kit\n",
    "import keras                                # for LSTM model to handle time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data prepared for ML\n",
    "\n",
    "Note: The whole process might take a few hours..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Clone the repo. Following code will create ~/data folder to contain ML train/test/val\n",
    "\n",
    "`git clone https://github.com/YerevaNN/mimic3-benchmarks/\n",
    "cd mimic3-benchmarks/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. The following command takes MIMIC-III CSVs, the steps takes around ~3 hour.\n",
    "- generates one directory per `SUBJECT_ID`, totally 33798 folders unnder ~/data/root/\n",
    "- writes ICU stay information to `data/{SUBJECT_ID}/stays.csv`\n",
    "- diagnoses to `data/{SUBJECT_ID}/diagnoses.csv`\n",
    "- and events to `data/{SUBJECT_ID}/events.csv`. \n",
    "\n",
    "`python -m mimic3benchmark.scripts.extract_subjects {PATH TO MIMIC-III CSVs} data/root/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. The following command attempts to fix some issues (ICU stay ID is missing) and removes the events that have missing information. About 80% of events remain after removing all suspicious rows.\n",
    "\n",
    "`python -m mimic3benchmark.scripts.validate_events data/root/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. The next command breaks up per-subject data into separate episodes (pertaining to ICU stays). \n",
    "- Time series of events are stored in `{SUBJECT_ID}/episode{#}_timeseries.csv` (where # counts distinct episodes) \n",
    "- episode-level information (patient age, gender, ethnicity, height, weight) and outcomes (mortality, length of stay, diagnoses) are stores in `{SUBJECT_ID}/episode{#}.csv`. \n",
    "- This script requires two files, one that maps event `ITEMIDs` to clinical variables and another that defines valid ranges for clinical variables (for detecting outliers, etc.). \n",
    "- Outlier detection is disabled in the current version.\n",
    "\n",
    "`python -m mimic3benchmark.scripts.extract_episodes_from_subjects data/root/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. The next command splits the whole dataset into training and testing sets. Note that the train/test split is the same of all tasks.\n",
    "\n",
    "`python -m mimic3benchmark.scripts.split_train_and_test data/root/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. The following commands will generate task-specific datasets, which can later be used in models. These commands are independent, if you are going to work only on one benchmark task, you can run only the corresponding command.\n",
    "\n",
    "\n",
    "1. early triage and risk assessment, i.e., mortality prediction <br>\n",
    "`python -m mimic3benchmark.scripts.create_in_hospital_mortality data/root/ data/in-hospital-mortality/`\n",
    "<br>\n",
    "    \n",
    "2. prediction of physiologic decompensation<br>\n",
    "`python -m mimic3benchmark.scripts.create_decompensation data/root/ data/decompensation/`\n",
    "<br>\n",
    "\n",
    "3. identification of high cost patients, i.e. length of stay forecasting<br>\n",
    "`python -m mimic3benchmark.scripts.create_length_of_stay data/root/ data/length-of-stay/`\n",
    "<br>\n",
    "\n",
    "4. characterization of complex, multi-system diseases, i.e., acute care phenotyping<br>\n",
    "`python -m mimic3benchmark.scripts.create_phenotyping data/root/ data/phenotyping/`<br>\n",
    "`python -m mimic3benchmark.scripts.create_multitask data/root/ data/multitask/`\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. After the above commands are done, there will be a directory data/{task} for each created benchmark task. These directories have two sub-directories: `train` and `test`. Each of them contains bunch of ICU stays and one file with name `listfile.csv`, which lists all samples in that particular set. \n",
    "\n",
    "Each row of listfile.csv has the following form: `icu_stay`, `period_length`, `label(s)`. A row specifies a sample for which the input is the collection of ICU event of `icu_stay` that occurred in the first `period_length` hours of the stay and the target is/are label(s). \n",
    "\n",
    "In in-hospital mortality prediction task `period_length` is always 48 hours, so it is not listed in corresponding listfiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. Seems some record missing for mortality data<br>\n",
    "Note remaining tasks also have data missing, won't copy all details.\n",
    "\n",
    "`(PY36) $ python -m mimic3benchmark.scripts.create_in_hospital_mortality data/root/ data/in-hospital-mortality/\n",
    "processed 5000 / 5070 patients\n",
    " 3236\n",
    "(length of stay is missing) 10128 episode1_timeseries.csv\n",
    "processed 100 / 28728 patients\n",
    "(length of stay is missing) 10168 episode1_timeseries.csv\n",
    "processed 2400 / 28728 patients\n",
    "(no events in ICU)  14219 episode1_timeseries.csv\n",
    "processed 2600 / 28728 patients\n",
    "(no events in ICU)  14469 episode1_timeseries.csv\n",
    "processed 4700 / 28728 patients\n",
    "(no events in ICU)  18350 episode1_timeseries.csv\n",
    "processed 5200 / 28728 patients\n",
    "(no events in ICU)  19097 episode1_timeseries.csv\n",
    "processed 5600 / 28728 patients\n",
    "(no events in ICU)  19872 episode1_timeseries.csv\n",
    "processed 16100 / 28728 patients\n",
    "(length of stay is missing) 499 episode1_timeseries.csv\n",
    "processed 28700 / 28728 patients\n",
    " 17903`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After these operations, `mimic3-benchmarks` folder will include below sub-folders, and every sub-folder will include datasets for `test` and `train` :\n",
    "    \n",
    "`decompensation` <br>\n",
    "`in-hospital-mortality` <br>\n",
    "`length-of-stay` <br>\n",
    "`multitask` <br>\n",
    "`phenotyping` <br>\n",
    "`root` <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to provide Train / Validation split before running the model:\n",
    "\n",
    "`python -m mimic3models.split_train_val {dataset-directory}`\n",
    "\n",
    "The `{dataset-directory}` can be either:\n",
    "\n",
    "`data/in-hospital-mortality\n",
    "data/decompensation\n",
    "data/length-of-stay\n",
    "data/phenotyping\n",
    "data/multitask`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the four main tasks:\n",
    "    1. Early triage and risk assessment, i.e., mortality prediction\n",
    "    2. Prediction of physiologic decompensation\n",
    "    3. Identification of high cost patients, i.e. length of stay forecasting\n",
    "    4. Characterization of complex, multi-system diseases, i.e., acute care phenotyping\n",
    "    \n",
    "    \n",
    "7 baselines are provided:<br>\n",
    "Note: Baseline models could take longer time because it's using grid search in pipeline.  \n",
    "\n",
    "    1. Linear/logistic regression\n",
    "    2. Standard LSTM\n",
    "    3. Standard LSTM + deep supervision\n",
    "    4. Channel-wise LSTM\n",
    "    5. Channel-wise LSTM + deep supervision\n",
    "    6. Multitask standard LSTM\n",
    "    7. Multitask channel-wise LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Logistic Regression Model test (In-hospital Mortality Prediction)\n",
    "Note: This would be a typical ML use case so chose to practise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`python -um mimic3models.in_hospital_mortality.logistic.main --l2 --C 0.001 --output_dir mimic3models/in_hospital_mortality/logistic`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Break down the model into Jupyter Notebook commands for better understanding</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add mimic3_benchmarks into system path so can import modules in its subdirectory\n",
    "# you can check system path by sys.path\n",
    "import sys\n",
    "from os.path import abspath, join, dirname\n",
    "sys.path.insert(0, join(os.path.abspath('./'), 'mimic3_benchmarks'))\n",
    "\n",
    "# import libraries\n",
    "from sklearn.preprocessing import Imputer, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "# import modules\n",
    "from mimic3_benchmarks.mimic3benchmark.readers import InHospitalMortalityReader\n",
    "from mimic3_benchmarks.mimic3models import common_utils\n",
    "from mimic3_benchmarks.mimic3models.metrics import print_metrics_binary\n",
    "from mimic3_benchmarks.mimic3models.in_hospital_mortality.utils import save_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define read feature and extract feature\n",
    "def read_and_extract_features(reader, period, features):\n",
    "    ret = common_utils.read_chunk(reader, reader.get_number_of_examples())\n",
    "    # ret = common_utils.read_chunk(reader, 100)\n",
    "    X = common_utils.extract_features_from_rawdata(ret['X'], ret['header'], period, features)\n",
    "    return (X, ret['y'], ret['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(args):  <class 'argparse.Namespace'>\n",
      "1.0\n",
      "C:\\Users\\ericx\\Jupyter Projects\\MGH_medical_AI\\mimic3_benchmarks/data/in-hospital-mortality/\n",
      "all\n",
      "True\n",
      ".\n",
      "all\n"
     ]
    }
   ],
   "source": [
    "# use arguments to parse the arments for command line running\n",
    "# we also can use default values to run in Jupyter Notebook\n",
    "# or use Grid search to targetly choose some parameters\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--C', type=float, default=1.0, help='inverse of L1 / L2 regularization')\n",
    "parser.add_argument('--l1', dest='l2', action='store_false')\n",
    "parser.add_argument('--l2', dest='l2', action='store_true')\n",
    "parser.set_defaults(l2=True)\n",
    "\n",
    "parser.add_argument('--period', type=str, default='all', \n",
    "                    help='specifies which period extract features from',\n",
    "                    choices=['first4days', 'first8days', 'last12hours', 'first25percent', 'first50percent', 'all'])\n",
    "parser.add_argument('--features', type=str, default='all', \n",
    "                    help='specifies what features to extract',\n",
    "                    choices=['all', 'len', 'all_but_len'])\n",
    "parser.add_argument('--data', type=str, \n",
    "                    help='Path to the data of in-hospital mortality task',\n",
    "                    default=os.path.join(os.path.abspath('./'), 'mimic3_benchmarks/data/in-hospital-mortality/'))\n",
    "parser.add_argument('--output_dir', type=str, \n",
    "                    help='Directory relative which all output files are stored',\n",
    "                    default='.')\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "print('type(args): ',type(args))\n",
    "print(args.C)\n",
    "print(args.data)\n",
    "print(args.features)\n",
    "print(args.l2)\n",
    "print(args.output_dir)\n",
    "print(args.period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data and extracting features ...\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "train_reader = InHospitalMortalityReader(dataset_dir=os.path.join(args.data, 'train'),\n",
    "                                             listfile=os.path.join(args.data, 'train_listfile.csv'),\n",
    "                                             period_length=48.0)\n",
    "\n",
    "val_reader = InHospitalMortalityReader(dataset_dir=os.path.join(args.data, 'train'),\n",
    "                                           listfile=os.path.join(args.data, 'val_listfile.csv'),\n",
    "                                           period_length=48.0)\n",
    "\n",
    "test_reader = InHospitalMortalityReader(dataset_dir=os.path.join(args.data, 'test'),\n",
    "                                            listfile=os.path.join(args.data, 'test_listfile.csv'),\n",
    "                                            period_length=48.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train data shape = (14681, 714)\n",
      "  validation data shape = (3222, 714)\n",
      "  test data shape = (3236, 714)\n"
     ]
    }
   ],
   "source": [
    "# extracting features, this is a longer process\n",
    "(train_X, train_y, train_names) = read_and_extract_features(train_reader, args.period, args.features)\n",
    "(val_X, val_y, val_names) = read_and_extract_features(val_reader, args.period, args.features)\n",
    "(test_X, test_y, test_names) = read_and_extract_features(test_reader, args.period, args.features)\n",
    "print('  train data shape = {}'.format(train_X.shape))\n",
    "print('  validation data shape = {}'.format(val_X.shape))\n",
    "print('  test data shape = {}'.format(test_X.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ericx\\AppData\\Local\\conda\\conda\\envs\\PY36\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# data compensation (for missing values)\n",
    "imputer = Imputer(missing_values=np.nan, strategy='mean', axis=0, verbose=0, copy=True)\n",
    "imputer.fit(train_X)\n",
    "train_X = np.array(imputer.transform(train_X), dtype=np.float32)\n",
    "val_X = np.array(imputer.transform(val_X), dtype=np.float32)\n",
    "test_X = np.array(imputer.transform(test_X), dtype=np.float32)\n",
    "\n",
    "# data normalization\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_X)\n",
    "train_X = scaler.transform(train_X)\n",
    "val_X = scaler.transform(val_X)\n",
    "test_X = scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "# note add 'lr' to store results under 'lr' folder to differ other model results\n",
    "penalty = ('l2' if args.l2 else 'l1')\n",
    "file_name = '{}.{}.{}.{}.C{}'.format('lr',args.period, args.features, penalty, args.C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original paramters from the article\n",
    "\n",
    "`LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
    "          n_jobs=None, penalty='l2', random_state=42, solver='warn',\n",
    "          tol=0.0001, verbose=0, warm_start=False)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'C': array([0.001, 0.01 , 0.1  , 1.   ]), 'penalty': ['l1', 'l2']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model, grid search can be time consuming\n",
    "# note article author state best model is under C = 0.001\n",
    "# This was the orginal training, used args.C for the C value\n",
    "    # logreg = LogisticRegression(penalty=penalty, C=args.C, random_state=42)\n",
    "    # logreg.fit(train_X, train_y)\n",
    "# Now, we will use Grid-search to test the best jypter parameters, instead of inputting by command line or default args\n",
    "grid={\"C\":np.logspace(-3,0,4), \"penalty\":[\"l1\", \"l2\"]}\n",
    "\n",
    "logr = LogisticRegression()\n",
    "logreg = GridSearchCV(logr, grid, cv = 10)\n",
    "logreg.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuned hpyerparameters :(best parameters)  {'C': 0.1, 'penalty': 'l1'}\n",
      "accuracy : 0.8806620802397657\n"
     ]
    }
   ],
   "source": [
    "# found best parameter combination is C=0.1 and penalty=l1\n",
    "# which differs from C=0.001 and panalty=l2 as article used, but we just observe here since difference are not that much in linear model\n",
    "print(\"tuned hpyerparameters :(best parameters) \",logreg.best_params_)\n",
    "print(\"accuracy :\",logreg.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results saved to: .\\results/In_hospital_Mortality \n",
      "\n",
      "The training data sets----------------\n",
      "confusion matrix:\n",
      "[[12395   299]\n",
      " [ 1349   638]]\n",
      "accuracy = 0.8877460956573486\n",
      "precision class 0 = 0.9018480777740479\n",
      "precision class 1 = 0.6808964610099792\n",
      "recall class 0 = 0.9764455556869507\n",
      "recall class 1 = 0.3210870623588562\n",
      "AUC of ROC = 0.8711970093301433\n",
      "AUC of PRC = 0.5690212947090911\n",
      "min(+P, Se) = 0.539738430583501\n",
      "\n",
      "\n",
      "The validation data sets----------------\n",
      "confusion matrix:\n",
      "[[2701   85]\n",
      " [ 304  132]]\n",
      "accuracy = 0.8792675137519836\n",
      "precision class 0 = 0.898835301399231\n",
      "precision class 1 = 0.6082949042320251\n",
      "recall class 0 = 0.9694902896881104\n",
      "recall class 1 = 0.302752286195755\n",
      "AUC of ROC = 0.8424099527783087\n",
      "AUC of PRC = 0.4926910001777761\n",
      "min(+P, Se) = 0.5114678899082569\n",
      "\n",
      "\n",
      "The testing data sets----------------\n",
      "confusion matrix:\n",
      "[[2785   77]\n",
      " [ 247  127]]\n",
      "accuracy = 0.8998764157295227\n",
      "precision class 0 = 0.9185356497764587\n",
      "precision class 1 = 0.6225489974021912\n",
      "recall class 0 = 0.973095715045929\n",
      "recall class 1 = 0.3395721912384033\n",
      "AUC of ROC = 0.847690743917159\n",
      "AUC of PRC = 0.4830230624406548\n",
      "min(+P, Se) = 0.4787234042553192\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating new directory to store results, under ./result/In_hospital_Mortality\n",
    "result_dir = os.path.join(args.output_dir, 'results/In_hospital_Mortality')\n",
    "common_utils.create_directory(result_dir)\n",
    "print('results saved to:', result_dir, '\\n')\n",
    "\n",
    "# predictions\n",
    "# output of logreg.predict_proba is a N x 2 matrix\n",
    "# output[:, 0] means the probability of prediction result 0 (alive)\n",
    "# output[:, 0] means the probability of prediction result 1 (dead)\n",
    "\n",
    "print('The training data sets----------------')\n",
    "prediction_train = logreg.predict_proba(train_X)\n",
    "with open(os.path.join(result_dir, 'lr_train_{}.json'.format(file_name)), 'w') as res_file:\n",
    "    ret = print_metrics_binary(train_y, prediction_train)\n",
    "    ret = {k : float(v) for k, v in ret.items()}\n",
    "    json.dump(ret, res_file)\n",
    "print('\\n')\n",
    "\n",
    "print('The validation data sets----------------')\n",
    "prediction_val = logreg.predict_proba(val_X)\n",
    "with open(os.path.join(result_dir, 'lr_val_{}.json'.format(file_name)), 'w') as res_file:\n",
    "    ret = print_metrics_binary(val_y, prediction_val)\n",
    "    ret = {k: float(v) for k, v in ret.items()}\n",
    "    json.dump(ret, res_file)\n",
    "print('\\n')\n",
    "\n",
    "print('The testing data sets----------------')\n",
    "prediction_test = logreg.predict_proba(test_X)[:, 1]\n",
    "with open(os.path.join(result_dir, 'lr_test_{}.json'.format(file_name)), 'w') as res_file:\n",
    "    ret = print_metrics_binary(test_y, prediction_test)\n",
    "    ret = {k: float(v) for k, v in ret.items()}\n",
    "    json.dump(ret, res_file)\n",
    "print('\\n')\n",
    "\n",
    "save_results(test_names, prediction, test_y, \n",
    "             os.path.join(args.output_dir, 'predictions/In_hospital_Mortality', file_name + '.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This is just a trial, as we know the normal linear model won't have good performance on time series data. LSTM will give much better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
